{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb79E6N7l4rD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkuX546h1-1"
      },
      "source": [
        "This code will remove the unwanted entries from the original dataset. Specifically, we add the number of recommendations, positive/negative votes, estimated owner count and total number of tags, then subtract number of non-english characters in the name and detailed description, and use this number to rank games. Non-english words are stigmatized because they are likely to interact negatively with the later algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3ZQcT6Wd7Ov"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import operator\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "cat_list = ['name', 'categories', 'short_description', 'tags', 'metacritic_score', 'positive', 'negative', 'header_image']\n",
        "\n",
        "# Regex pattern to remove unusual symbols (superscripts, copyright, ®, ™, etc.)\n",
        "# Keep basic punctuation, letters, digits, and whitespace\n",
        "clean_pattern = re.compile(r'[^\\w\\s.,!?;:()\\-\\']')  # remove anything except word chars, whitespace, and common punctuation\n",
        "\n",
        "data = None\n",
        "with open('data/games.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "scored_games = []\n",
        "\n",
        "for app_id, game_data in data.items():\n",
        "    # Compute base score\n",
        "    score = game_data.get('recommendations', 0) + game_data.get('positive', 0) + game_data.get('negative', 0)\n",
        "\n",
        "    # Add estimated owners\n",
        "    estimated_owners_str = game_data.get('estimated_owners', '0 - 0').replace(',', '')\n",
        "    try:\n",
        "        owners_range = [int(x.strip()) for x in estimated_owners_str.split('-')]\n",
        "        if len(owners_range) == 2:\n",
        "            score += sum(owners_range) / 2\n",
        "        elif len(owners_range) == 1:\n",
        "            score += owners_range[0]\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    # Add tag values\n",
        "    tags = game_data.get('tags', {})\n",
        "    if isinstance(tags, dict):\n",
        "        score += sum(tags.values())\n",
        "    elif isinstance(tags, list):\n",
        "        score += len(tags)\n",
        "\n",
        "    # Count non-English characters\n",
        "    non_english_char_count = 0\n",
        "    name = game_data.get('name', '')\n",
        "    detailed_description = game_data.get('detailed_description', '')\n",
        "    non_english_char_count += len(re.findall(r'[^\\x00-\\x7F]+', name))\n",
        "    non_english_char_count += len(re.findall(r'[^\\x00-\\x7F]+', detailed_description))\n",
        "    score -= non_english_char_count * 0.1\n",
        "\n",
        "    # Clean text fields\n",
        "    for key in ['name', 'short_description']:\n",
        "        if key in game_data and isinstance(game_data[key], str):\n",
        "            game_data[key] = clean_pattern.sub('', game_data[key])\n",
        "\n",
        "    # Clean tags if dictionary or list\n",
        "    if 'tags' in game_data:\n",
        "        if isinstance(game_data['tags'], dict):\n",
        "            cleaned_tags = {}\n",
        "            for t, val in game_data['tags'].items():\n",
        "                clean_tag = clean_pattern.sub('', t)\n",
        "                cleaned_tags[clean_tag] = val\n",
        "            game_data['tags'] = cleaned_tags\n",
        "        elif isinstance(game_data['tags'], list):\n",
        "            game_data['tags'] = [clean_pattern.sub('', t) for t in game_data['tags']]\n",
        "\n",
        "    # Clean categories\n",
        "    if 'categories' in game_data and isinstance(game_data['categories'], list):\n",
        "        game_data['categories'] = [clean_pattern.sub('', c) for c in game_data['categories']]\n",
        "\n",
        "    scored_games.append((app_id, score))\n",
        "\n",
        "# Sort and filter top 100k unique games\n",
        "scored_games.sort(key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "filtered_data = {}\n",
        "added_names = set()\n",
        "unique_game_count = 0\n",
        "\n",
        "for app_id, score in scored_games:\n",
        "    game = data[app_id]\n",
        "    game_name_lower = game.get('name', '').lower()\n",
        "    if game_name_lower in added_names:\n",
        "        continue\n",
        "    if unique_game_count < 100000:\n",
        "        new_game_data = {key: game[key] for key in cat_list if key in game}\n",
        "        filtered_data[app_id] = new_game_data\n",
        "        added_names.add(game_name_lower)\n",
        "        unique_game_count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "with open('modified_games.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(filtered_data, file, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following will vectorize the corpus associated with each game, using TF-IDF, and then reduce dimensions of the resulting feature matrix via svd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c346c22",
        "outputId": "0453c356-8182-4a0a-ef19-f247df7fa80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 100000 games...\n",
            "Processed 10000 games...\n",
            "Processed 20000 games...\n",
            "Processed 30000 games...\n",
            "Processed 40000 games...\n",
            "Processed 50000 games...\n",
            "Processed 60000 games...\n",
            "Processed 70000 games...\n",
            "Processed 80000 games...\n",
            "Processed 90000 games...\n",
            "Processed 100000 games...\n",
            "Vectorizing with TF-IDF...\n",
            "TF-IDF matrix shape: (100000, 10000)\n",
            "TF-IDF matrix size: 17.30 MB (sparse)\n",
            "Applying TruncatedSVD...\n",
            "Reduced matrix shape: (100000, 600)\n",
            "Explained variance ratio: 0.4021\n",
            "Saving to parquet...\n",
            "Done! Saved 100000 games with 600 dimensions\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "DATA_DIR = 'preprocessing/data/'\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Write processed text to a temporary file\n",
        "temp_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, encoding='utf-8')\n",
        "temp_file_path = temp_file.name\n",
        "game_names = []  # Store game names separately (much smaller than full text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      with open(DATA_DIR + 'modified_games.json', 'r', encoding='utf-8') as file:\n",
        "          dataset = json.load(file)\n",
        "\n",
        "      print(f\"Processing {len(dataset)} games...\")\n",
        "\n",
        "      for idx, app in enumerate(dataset):\n",
        "          game = dataset[app]\n",
        "\n",
        "          name = game['name']\n",
        "          game_names.append(name)\n",
        "\n",
        "          # Build the text representation\n",
        "          text_parts = [name.lower()]*3\n",
        "\n",
        "          description = game['short_description']*3\n",
        "          text_parts.append(clean_text(description))\n",
        "\n",
        "          categories = game['categories']\n",
        "          text_parts.extend([c.lower() for c in categories])\n",
        "\n",
        "          tags = game['tags']\n",
        "          counter = 0\n",
        "          if isinstance(tags, dict):\n",
        "              for tag_name, tag_num in tags.items():\n",
        "                  text_parts.append(tag_name.lower())\n",
        "                  counter += 1\n",
        "                  if counter == 5:\n",
        "                      break\n",
        "          elif isinstance(tags, list):\n",
        "              for tag_name in tags:\n",
        "                  if isinstance(tag_name, str):\n",
        "                      text_parts.append(tag_name.lower())\n",
        "                      counter += 1\n",
        "                      if counter == 5:\n",
        "                          break\n",
        "\n",
        "          temp_file.write(' '.join(text_parts) + '\\n')\n",
        "\n",
        "          if (idx + 1) % 10000 == 0:\n",
        "              print(f\"Processed {idx + 1} games...\")\n",
        "\n",
        "      temp_file.close()\n",
        "\n",
        "      print(\"Vectorizing with TF-IDF...\")\n",
        "      # Limit vocabulary size to reduce memory\n",
        "      vectorizer = TfidfVectorizer(\n",
        "          stop_words='english',\n",
        "          max_features=10000,  # Limit to top 10k features\n",
        "          max_df=0.75,  # Ignore terms in >75% of docs\n",
        "          min_df=2     # Ignore terms in <2 docs\n",
        "      )\n",
        "\n",
        "      with open(temp_file_path, 'r', encoding='utf-8') as f:\n",
        "          X = vectorizer.fit_transform(f)\n",
        "\n",
        "      print(f\"TF-IDF matrix shape: {X.shape}\")\n",
        "      print(f\"TF-IDF matrix size: {X.data.nbytes / (1024**2):.2f} MB (sparse)\")\n",
        "\n",
        "      # Apply TruncatedSVD for dimensionality reduction\n",
        "      print(\"Applying TruncatedSVD...\")\n",
        "      n_components = 600  # Reduce to 100 dimensions (adjust as needed)\n",
        "      svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "      X_reduced = svd.fit_transform(X)\n",
        "\n",
        "      print(f\"Reduced matrix shape: {X_reduced.shape}\")\n",
        "      print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "      # Now save as parquet - much smaller and stays dense\n",
        "      df = pd.DataFrame(\n",
        "          X_reduced,\n",
        "          index=game_names,\n",
        "          columns=[f'component_{i}' for i in range(n_components)]\n",
        "      )\n",
        "\n",
        "      print(\"Saving to parquet...\")\n",
        "      df.to_parquet('games_vectors.parquet')\n",
        "\n",
        "      print(f\"Done! Saved {len(game_names)} games with {n_components} dimensions\")\n",
        "\n",
        "  finally:\n",
        "      # Clean up the temporary file\n",
        "      if os.path.exists(temp_file_path):\n",
        "          os.unlink(temp_file_path)\n",
        "\n",
        "  # Save the vectorizer and svd objects\n",
        "      import joblib\n",
        "\n",
        "      joblib.dump(vectorizer, DATA_DIR + 'tfidf_vectorizer.pkl')\n",
        "      joblib.dump(svd, DATA_DIR + 'svd_model.pkl')\n",
        "      joblib.dump(lemmatizer, DATA_DIR + 'lemmatizer.pkl')\n",
        "      joblib.dump(stop_words, DATA_DIR + 'stop_words.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cwCb65C4ib2"
      },
      "source": [
        "Sanity checks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a283edf1",
        "outputId": "b0ecbf87-a27b-42b2-aa24-ac943750dd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique game names: 100000\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# Load the modified games data\n",
        "with open('modified_games.json', 'r', encoding='utf-8') as file:\n",
        "    dataset = json.load(file)\n",
        "\n",
        "# Create a list of lowercase game names\n",
        "game_names = [game['name'].lower() for game in dataset.values()]\n",
        "\n",
        "# Count the occurrences of each game name\n",
        "name_counts = Counter(game_names)\n",
        "\n",
        "# The number of unique names is the number of entries in name_counts\n",
        "num_unique_names = len(name_counts)\n",
        "\n",
        "print(f\"Number of unique game names: {num_unique_names}\")\n",
        "\n",
        "# Find names that appear more than once (This part is kept for completeness,\n",
        "# although based on previous output there should be none)\n",
        "repeated_names = {name: count for name, count in name_counts.items() if count > 1}\n",
        "\n",
        "# Display the repeated names and their counts\n",
        "if repeated_names:\n",
        "    print(\"\\nRepeated game names (lowercase) and their counts:\")\n",
        "    for name, count in repeated_names.items():\n",
        "        print(f\"- {name}: {count}\")\n",
        "# else:\n",
        "    # The message \"No repeated game names found...\" is now less relevant since we print the unique count\n",
        "    # print(\"No repeated game names found after converting to lowercase.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF6K_LeYIG5Y",
        "outputId": "c83a5e5c-8437-45ab-d596-f514d14a197e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100000, 600)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = pd.read_parquet('games_vectors.parquet')\n",
        "print(df.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
